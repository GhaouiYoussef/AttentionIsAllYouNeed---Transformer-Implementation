{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c06380-178e-48c3-ba3a-c720b2d7fadb",
   "metadata": {},
   "source": [
    "# Building the Transformer Architecture from Attention-is-all-you-need Paper\n",
    "<html><img src=\"C:\\Users\\THiNKBooK\\Downloads\\transformer achitectire.png\"></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada785f0-ea66-455c-bd5e-ca8255b256ec",
   "metadata": {},
   "source": [
    "## Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14228632-b4a7-4d6f-89ee-a49bb50544ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45293bf-7389-4d74-90c0-1427344e79f7",
   "metadata": {},
   "source": [
    "## 1. Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
    "### 1.1 Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3f361-a5cb-4910-94d5-a3763df1a684",
   "metadata": {},
   "source": [
    "<html><img src=\"C:\\Users\\THiNKBooK\\Downloads\\Figure_1_Multi_Head_Attention_source_image_created_by_author_653bad32f1.avif\"></html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3e615-dc5b-4a95-b6c0-933af826caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        '''\n",
    "        The class is defined as a subclass of PyTorch's nn.Module.\n",
    "    \n",
    "            d_model: Dimensionality of the input.\n",
    "            num_heads: The number of attention heads to split the input into.\n",
    "        '''\n",
    "    \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads AKA subjects tp process\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "        Calculating Attention Scores: attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n",
    "        Applying Mask: If a mask is provided, it is applied to the attention scores to mask out specific values.\n",
    "        Calculating Attention Weights: The attention scores are passed through a softmax function to convert them into probabilities that sum to 1.\n",
    "        Calculating Output: The final output of the attention is calculated by multiplying the attention weights by the values (V).\n",
    "        '''\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # we need to transpose because usualy we have k.shape(d_model, d_k) and Q.shape(d_model, d_Q)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "'''\n",
    "This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
    "'''\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "'''\n",
    "After applying attention to each head separately, this method combines the results back into a single tensor of shape (batch_size, seq_length, d_model). This prepares the result for further processing.\n",
    "'''\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "'''\n",
    "The forward method is where the actual computation happens:\n",
    "\n",
    "Apply Linear Transformations: The queries (Q), keys (K), and values (V) are first passed through linear transformations using the weights defined in the initialization.\n",
    "Split Heads: The transformed Q, K, V are split into multiple heads using the split_heads method.\n",
    "Apply Scaled Dot-Product Attention: The scaled_dot_product_attention method is called on the split heads.\n",
    "Combine Heads: The results from each head are combined back into a single tensor using the combine_heads method.\n",
    "Apply Output Transformation: Finally, the combined tensor is passed through an output linear transformation.\n",
    "'''\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230fc80-ef75-4730-b631-4e7ef9eeed9e",
   "metadata": {},
   "source": [
    "### 1.2 Position-wise Feed-Forward Networks\n",
    "- In summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d6babd-ffcd-46b7-83cc-a6dcb300df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        '''\n",
    "        d_model: Dimensionality of the model's input and output.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        self.fc1 and self.fc2: Two fully connected (linear) layers with input and output dimensions as defined by d_model and d_ff.\n",
    "        self.relu: ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers.\n",
    "        '''\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: The input to the feed-forward network.\n",
    "        self.fc1(x): The input is first passed through the first linear layer (fc1).\n",
    "        self.relu(...): The output of fc1 is then passed through a ReLU activation function. ReLU replaces all negative values with zeros, introducing non-linearity into the model.\n",
    "        self.fc2(...): The activated output is then passed through the second linear layer (fc2), producing the final output.\n",
    "        '''\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05fdfa-67ca-4450-b4aa-f6939a9fd906",
   "metadata": {},
   "source": [
    "### 1.3 Positional Encoding\r\n",
    "\r\n",
    "- **Positional Encoding** is used to inject positional information of each token in the input sequence. It leverages sine and cosine functions with different frequencies to compute the positional encodings.\r\n",
    "- Unlike CNNs and RNNs, the self-attention mechanism used in Transformers enables parallel computation but lacks inherent word order information. To address this, **positional encoding** is used to provide positional context to the model. Specifically, a position-dependent signal is added to each word embedding in the input sequence. This helps the model incorporate the order of words in the sequence.\r\n",
    "- The output of positional encoding has the same dimension as the embedding layer, allowing it to be added directly to the word embeddings. This ensures that both positional information (from positional encoding) and semantic information (from embeddings) are integrated and passed to subsequent layers.\r",
    "#\n",
    "\r\n",
    "### Variants of Positional Encoding\r\n",
    "\r\n",
    "There are several approaches to implementing positional encoding, but the original Transformer model uses sine and cosine functions as defined in the following eq\n",
    "\n",
    "\n",
    "$$\\\n",
    "  PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$$$$\\\n",
    "  PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "he benefits of parallel computation.\r\n",
    "\r\n",
    "P\r\n",
    "E\r\n",
    "p\r\n",
    "o\r\n",
    "s\r\n",
    "+\r\n",
    "k\r\n",
    " can be represented as a linear function of \r\n",
    "P\r\n",
    "E\r\n",
    "p\r\n",
    "o\r\n",
    "s\r\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30593ef9-fde0-472f-afa3-277fcf4ee478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        '''\n",
    "        - d_model: The dimension of the model's input.\n",
    "        \n",
    "        - max_seq_length: The maximum length of the sequence for which positional encodings are pre-computed.\n",
    "        \n",
    "        - pe: A tensor filled with zeros, which will be populated with positional encodings.\n",
    "        \n",
    "        - position: A tensor containing the position indices for each position in the sequence.\n",
    "        \n",
    "        - div_term: A term used to scale the position indices in a specific way.\n",
    "        \n",
    "        - The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
    "        \n",
    "            * They provide unique encodings for each position.\n",
    "            * They ensure that the encodings for positions close to each other (e.g., 3 and 4) are also close in value, preserving some notion of \"relative\" position.\n",
    "            * They are periodic, which helps the model generalize to sequences longer than those it was trained on.\n",
    "            \n",
    "        - Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a9dee-b675-4ba5-9da3-e48fb06e5465",
   "metadata": {},
   "source": [
    "## 2. Building the Encoder Blocks\n",
    "- The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model.\n",
    "\n",
    "<html><img src=\"C:\\Users\\THiNKBooK\\Downloads\\Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.avif\"></html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8872b214-8830-417c-ac72-43a9d773b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        '''\n",
    "        d_model: The dimensionality of the input.\n",
    "        num_heads: The number of attention heads in the multi-head attention.\n",
    "        d_ff: The dimensionality of the inner layer in the position-wise feed-forward network.\n",
    "        dropout: The dropout rate used for regularization.\n",
    "        \n",
    "        Components:\n",
    "        \n",
    "        self.self_attn: Multi-head attention mechanism.\n",
    "        self.feed_forward: Position-wise feed-forward neural network.\n",
    "        self.norm1 and self.norm2: Layer normalization, applied to smooth the layer's input.\n",
    "        self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n",
    "        '''\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        '''\n",
    "        - Self-Attention: The input x is passed through the multi-head self-attention mechanism.\n",
    "        - Add & Normalize (after Attention): The attention output is added to the original input (residual connection), followed by dropout and normalization using norm1.\n",
    "        - Feed-Forward Network: The output from the previous step is passed through the position-wise feed-forward network.\n",
    "        - Add & Normalize (after Feed-Forward): Similar to step 2, the feed-forward output is added to the input of this stage (residual connection), followed by dropout and normalization using norm2.\n",
    "        - Output: The processed tensor is returned as the output of the encoder layer.\n",
    "        '''\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39c5b1-e444-479a-a14b-f0d756c29b0f",
   "metadata": {},
   "source": [
    "## 3. Building the Decoder Blocks\n",
    "-The DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model.\n",
    "\n",
    "<html><img src=\"C:\\Users\\THiNKBooK\\Downloads\\Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.avif\"></html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a69ffc5a-4b11-4076-a0c8-22dfd262c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        '''\n",
    "        d_model: The dimensionality of the input.\n",
    "        num_heads: The number of attention heads in the multi-head attention.\n",
    "        d_ff: The dimensionality of the inner layer in the feed-forward network.\n",
    "        dropout: The dropout rate for regularization.\n",
    "        \n",
    "        Components:\n",
    "        \n",
    "        self.self_attn: Multi-head self-attention mechanism for the target sequence.\n",
    "        self.cross_attn: Multi-head attention mechanism that attends to the encoder's output.\n",
    "        self.feed_forward: Position-wise feed-forward neural network.\n",
    "        self.norm1, self.norm2, self.norm3: Layer normalization components.\n",
    "        self.dropout: Dropout layer for regularization.\n",
    "        '''\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        '''\n",
    "        x: The input to the decoder layer.\n",
    "        enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "        src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "        tgt_mask: Target mask to ignore certain parts of the decoder's input.\n",
    "        \n",
    "        Processing Steps:\n",
    "        \n",
    "        - Self-Attention on Target Sequence: The input x is processed through a self-attention mechanism.\n",
    "        \n",
    "        - Add & Normalize (after Self-Attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n",
    "        \n",
    "        - Cross-Attention with Encoder Output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n",
    "        \n",
    "        - Add & Normalize (after Cross-Attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n",
    "        \n",
    "        - Feed-Forward Network: The output from the previous step is passed through the feed-forward network.\n",
    "        \n",
    "        - Add & Normalize (after Feed-Forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n",
    "        \n",
    "        - Output: The processed tensor is returned as the output of the decoder layer.\n",
    "        '''\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d3d13-354b-40b2-9a9a-fc67066d8158",
   "metadata": {},
   "source": [
    "## 4. Combining the Encoder and Decoder layers to create the complete Transformer network\n",
    "<html><img src=\"C:\\Users\\THiNKBooK\\Downloads\\transformer achitectire.png\"></html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dc598dc-1998-43c6-8675-ad9719643f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        '''\n",
    "        src_vocab_size: Source vocabulary size.\n",
    "        tgt_vocab_size: Target vocabulary size.\n",
    "        d_model: The dimensionality of the model's embeddings.\n",
    "        num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "        num_layers: Number of layers for both the encoder and the decoder.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        max_seq_length: Maximum sequence length for positional encoding.\n",
    "        dropout: Dropout rate for regularization.\n",
    "        \n",
    "        And it defines the following components:\n",
    "        \n",
    "        - self.encoder_embedding: Embedding layer for the source sequence.\n",
    "        - self.decoder_embedding: Embedding layer for the target sequence.\n",
    "        - self.positional_encoding: Positional encoding component.\n",
    "        - self.encoder_layers: A list of encoder layers.\n",
    "        - self.decoder_layers: A list of decoder layers.\n",
    "        - self.fc: Final fully connected (linear) layer mapping to target vocabulary size.\n",
    "        - self.dropout: Dropout layer.\n",
    "        '''\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        '''\n",
    "        - Input Embedding and Positional Encoding: The source and target sequences are first embedded using their respective embedding layers and then added to their positional encodings.\n",
    "        - Encoder Layers: The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n",
    "        - Decoder Layers: The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n",
    "        - Final Linear Layer: The decoder's output is mapped to the target vocabulary size using a fully connected (linear) layer.\n",
    "        '''\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84703324-9f51-473b-9d4b-d85322743f3f",
   "metadata": {},
   "source": [
    "# Training the PyTorch Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "007e9733-4b2a-4255-88f6-4a86c6203f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10c29e39-d044-4597-9273-001543abaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3a17176-d97f-4520-9784-29c3b8d8d912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(5000, 512)\n",
       "  (decoder_embedding): Embedding(5000, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = transformer(src_data, tgt_data[:, :-1])\n",
    "#     loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc9d54-ee90-4528-bc22-066e61c9765a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
